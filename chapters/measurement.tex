 % vim: tw=80

\chapter[Measurement of the Triple-Differential Dijet Cross Section]{Measurement
of the Triple-Differential\\ Dijet Cross Section}
\label{sec:measurement}

The preceeding chapter laid the foundation for the triple-differential dijet
cross section measurement by presenting the NLO pQCD calculations for this
observable and demonstrating the PDFs sensitivity. The sizable PDF uncertainties
especially in phase space regions involving boosted dijet topologies indicate
that a sufficiently precise measurement in these phase space regions could be
used to better constrain the PDFs. 

This chapter is dedicated to the measurement of the cross section from collision
data recorded by CMS during the 2012 LHC run at a center-of-mass energy of
\SI{8}{\TeV}. The first two sections give the definition of the measured cross
section and the analyzed data sets. Sec.~\ref{sec:event_selection} describes the
event selection, in which clean dijet events are identified. Based on simulated
events, the detector response is studied and the jet energy resolution derived
which is used subsequently to correct the measured cross section for detector
effects in an iterative unfolding procedure. A detailed study of all
experimental sources of uncertainty in Sec.~\ref{sec:experimental_uncertainties}
concludes the measurement.

Finally, the comparison of unfolded data to NLO predictions is presented in
Sec.~\ref{sec:nlo_comparisons}. Furthermore, comparisons to NLO calculations
complemented with matched parton showers, as made possible with the recently
released Herwig~7 MC event generator, are shown.
~\todo{mention powheg if ready}

\section{Cross Section Definition}

The observed dijet event yields are transformed into a triple-differrential
cross section, which is defined as
%
\begin{equation*}
    \frac{d^3 \sigma}{d \ptavg d \ystar d \yboost} = \frac{1}{\epsilon
        \mathcal{L}_{\mathrm{int, eff}}} \frac{N}{\Delta \ptavg \Delta \ystar
        \Delta \yboost},
\end{equation*}
%
where $N$ denotes the number of dijet events, $\mathcal{L}_{\mathrm{int, eff}}$
the effective integrated luminosity and $\epsilon$ the product of trigger and
event selection efficiencies which are greater than 99\% in the measured phase
space. The cross section is normalized by the widths of the \ptavg, \ystar, and
\yboost bins. 

The size of the \ptavg bins is somewhat larger than the observed resolution for
\ptavg at the bin center. For simplification, the same \ptavg binning is chosen
as for the inclusive jet cross section measurement, which has a slightly worse
resolution. The equidistant \ystar and \yboost bins start from zero and go up to
three with a bin size of one.

\section{Data samples}
\label{sec:datasets}

The measurement is based on data collected by CMS in the 2012 run
period of the LHC at a center-of-mass energy of \SI{8}{\TeV}. The accumulated
data corresponds to a total integrated luminosity of $\SI{19.71}{\fbinv}$. 

The 2012 LHC data taking is subdivided into four periods A, B, C and D and the
data sets are split accordingly into separate samples. Each data set is further
split into subsets containing only a fraction of all triggered events which are
grouped according to the physics purpose. This analysis relies on a range of jet
triggers. All events triggered by prescaled jet triggers were streamed into the
data sets \texttt{Jet} and \texttt{JetMon} while the events of unprescaled jet
triggers were moved into the data set \texttt{JetHT}. As this assignment changed
over the 2012 run periods, it was taken care to only select the correct subset
of events. Table~\ref{tab:data:datasets} presents the CMS collision data sets
analyzed in this thesis together with the integrated luminosity collected in
each run period.

\begin{table}[htbp]
    \centering
    \caption[Datasets of the 2012 LHC run period]
       {The 2012 collision data of CMS comprises four data sets collected in the run periods
           A, B, C and D. The total integrated luminosity of all data collected
           in 2012 sums up to \SI{19.71}{\fbinv}.}
    \label{tab:data:datasets}
    \begin{tabular}{cccc}
    \toprule
    \textbf{Run}  & \textbf{run range} & \textbf{data set}                       & \textbf{luminosity}\\
                  &                    &                                        & \si{\fbinv}\\\midrule
    A             & 190456--193621     & /Jet/Run2012A-22Jan2013-v1/AOD         & \num{0.88}\\
    B             & 193834--196531     & /Jet[Mon,HT]/Run2012B-22Jan2013-v1/AOD & \num{4.49}\\
    C             & 198022--203742     & /Jet[Mon,HT]/Run2012C-22Jan2013-v1/AOD & \num{7.06}\\
    D             & 203777--208686     & /Jet[Mon,HT]/Run2012D-22Jan2013-v1/AOD & \num{7.37}\\
    \bottomrule
    \end{tabular}
\end{table}
\todo{check for rounding errors}

\subsection{Monte Carlo Samples}

To compare the measured data with simulated events, two
Monte-Carlo event generators were used, which are described in
Sec.~\ref{subsection:mc_generators}. Each MC generator is utilizing an optimized
tune to simulate the underlying event. Additionally, the Monte Carlo event samples
contain not only the hard scattering event, but also also an admixture
simulating the pileup collisions observed in real collisions.

The Madgraph Monte Carlo sample is generated with a multi-leg improved QCD
process. By using Madgraph, the LO matrix elements do not only contain the $2
\rightarrow 2$ matrix elements, but also multi-leg matrix elements which better
describe multi-jet events, but have to be matched to the subsequent parton
shower. The underlying event is modeled using the tune Z2$^*$. The parton shower
and hadronization is carried out using the Pythia~6 event generator which is
interfaced to Madgraph by the LHE event record~\cite{Alwall:2006yp}. This Monte
Carlo sample is the main sample used for data comparisons, not because of the
multileg-improved matrix elements which are not as important for this dijet
analysis, but because of the huge number of events generated and simulated.

Furthermore, a data sample generated with Pythia~8 is used. While including only the $2
\rightarrow 2$ LO matrix elements, the event simulation and the underlying event
tune are improved in this newer version of the Pythia Monte Carlo generator.

To be able to perform comparisons between data and simulated events on
reconstructed level, events from both Monte Carlo generators were propagated
through the complete simulation of the CMS detector. This allows for further
studies like the measurement of the jet transverse momentum resolution which are
needed to finally correct the measurement for detector effects.

Predictions of jet cross sections, especially cross sections involving the jet
transverse momenta, are notoriously difficult because of the steeply falling
spectrum. Consequently, it isn't possible to generate enough events to populate
also the highest-\pt phase space regions with a sufficiently large number of
events. However, there exist two approaches to overcome this issue. The first
method involves the reweighting of the generation process to generate more
events at high transverse momentum. Unfortunately, this spoils the absolute
normalization of the cross section as well as it involves the consideration of
huge event weights. The second method is based on splitting the phase space in
multiple regions, in which events are generated separately. The
different phase space regions are then stitched together later in the data
analysis while taking into account the cross sections in the different phase
space regions. The Madgraph sample is split into four regions according to the
scalar sum of the transverse momenta, $H_{\mathrm{T}}$, while the Pythia 8
sample is split into twelve data sets according to the transverse momentum of
the leading jet. Table~\ref{tab:mc_samples} shows all Monte Carlo data sets
as well as the cross section and the generated number of events.

\section{Event selection}
\label{sec:event_selection}

The selection of events is based on several quality criteria recommended by CMS
ore developed specifically for this analysis to yield clean dijet events.
Furthermore, phase spaces cuts on the jets ensure the applicability and comparibility of NLO
theory calculations.

\subsection{Certified Data Selection}

The first step in the event processing chain is to only pick data from runs and
luminosity sections which fullfill certain criteria. These criteria include
proper performance of all detector subsystems as well as the passing of data
quality monitoring (DQM) steps during the validation process. The good
lumisections within a run are announced using a JSON data file, called
\textsc{Golden JSON}. The applied lumisection certification
file\footnote{\texttt{Cert\_190456-208686\_8TeV\_22Jan2013ReReco\_Collisions12\_JSON}}
in this analysis is based on the final event reconstruction of the 2012 data
sets.

\subsection{Trigger Selection}

To measure and reconstruct the \ptavg spectrum of dijet production, a set of
single jet triggers has been used. Single jet triggers consist of one L1 trigger
seed and multiple HLT filters. The L1 trigger has a lower threshold to ensure
full efficiency versus \pt of the HLT trigger. Since the \pt spectrum is steeply
falling and the rates for low-\pt jets are very high, it is not feasible to use
a single unprescaled trigger for the selection of all interesting events.
Instead, a set of five prescaled low-\pt trigger paths, each with different
prescale value, is used to collect sufficient data in the lower part of the \pt
spectrum.  Additionally, one unprescaled trigger is used in the \pt-region in
which the rate is sufficiently small, \ie at high \pt, to collect and store all
events. A prescale value of $n$ means, that only every $n$-th event which is accepted by
a trigger, is kept. Of course, the applied prescale values have to be taken into
account when the original spectrum is reconstructed. 

\begin{table}[htbp]
    \centering
    \caption[Single Jet Trigger Paths]{List of all single jet trigger paths used in the analysis. Each
        trigger is employed in an mutually exclusive phase space in \ptavg in which
        the trigger exhibits an efficiency larger than 99\%. The column
        $p_{\mathrm{T},99\%}$ indicates the value a which each trigger reaches
        that value. The last column reports the effective luminosity seen by
        each trigger. This number, divided by the total integrated luminosity of
        \SI{19.71}{\fbinv} gives the effective prescale applied on a trigger
        over the whole run period.}
    \label{tab:triggers}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Trigger path}        & \textbf{L1 threshold} & \textbf{HLT threshold} & \boldmath$p_{\mathrm{T},99\%}$ & \textbf{Eff. Lumi} \\
                                     & \si{\GeV}             & \si{\GeV}              & \si{\GeV}              & \si{\fbinv}\\\midrule
                      HLT\_PFJet40   & 16                    & 40                     & --                     & \num{0.8e-4}\\
                      HLT\_PFJet80   & 36                    & 80                     & 123                    & \num{0.21e-2}\\
                      HLT\_PFJet140  & 68                    & 140                    & 192                    & \num{0.56e-1}\\
                      HLT\_PFJet200  & 92                    & 200                    & 263                    & \num{0.26}\\
                      HLT\_PFJet260  & 128                   & 260                    & 353                    & \num{1.06}\\
                      HLT\_PFJet320  & 128                   & 320                    & 412                    & \num{19.71}\\
        \bottomrule
    \end{tabular}
\end{table}


Table~\ref{tab:triggers} shows all single jet triggers used in the data
analysis. Each trigger is used in mutually exclusive regions of \ptavg, in which
the trigger is fully efficient.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/trigger_eff_HLT_PFJET80_default.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/trigger_eff_HLT_PFJET140_default.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/trigger_eff_HLT_PFJET200_default.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/trigger_eff_HLT_PFJET260_default.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/trigger_eff_HLT_PFJET320_default.pdf}\hfill
    \caption[Turn-on curves of single jet HLT trigger paths]{Trigger turn-on curves for the single jet trigger
    paths used in the analysis. To determine the 99\% efficiency threshold, the
    trigger paths are fitted using a sigmoid function taking into account the
    uncertainties using Clopper-Pearson confidence intervals.}
    \label{fig:trigger_eff}
\end{figure}

The jet reconstruction algorithms and the jet energy corrections applied on HLT
level are slightly different compared to the finally reconstructed objects used
in this analysis. Futhermore, the efficiency of each trigger is not calculated
as a function of jet \pt, which was used in the trigger decision, but versus
\ptavg of the two leading jets as used in this analysis. Therefore, the triggers
show a turn-on behaviour, as can been seen in Fig.~\ref{fig:trigger_eff}.
Consequently, it is neccessary to determine the threshold when a trigger becomes
fully efficient, which is defined to be the value at which the efficiency is
larger than 99\%.

Basically, it is possible to calculate the efficiency of a given trigger by
dividing the number of passing events through the number of events which pass
the next-lower trigger in \pt, as the looser trigger is by definition efficient,
when the higher trigger becomes efficient. This is aggravated through the
different prescales applied to each trigger path. While it is possible to
normalize the yield by the effective luminosity seen by each trigger, this
method is affected by larger statistiscal fluctuations as the number of events
is very different between two trigger paths.

Therefore, a more challenging but superior method is employed. When the L1
and HLT triggers are processed, the objects, \ie the jet four-vectors, on
which the trigger decision is based, are stored. Thus, it is possible to
re-emulate the trigger decision by comparing the transverse momentum of the L1
trigger object  with the L1 threshold and the \pt of the HLT trigger object with
the HLT threshold~\cite{Stober:2012abc}.

Similarly, the trigger decision of the next-higher trigger can be emulated
starting from a lower trigger path. A set of events $S_1 = \left\{E_i | T_A
(E_i) = \mathrm{true} \right\}$ which was accepted  by the lower trigger path $T_A$ is
used to determine the subset $S_2 = \left\{E_i|T_A(E_i) \wedge  T_B(E_i)
\right\}$ which also passes the next-higher trigger $T_B$, see
Eq.~\ref{eq:trigger_eff}. The ratio of both event sets is used to determine
the turn-on curve as shown for each trigger path in Fig.~\ref{fig:trigger_eff}. 
The uncertainty on the efficiency is
indicated by errorbars which represent Clopper-Pearson confidence intervals.

\begin{equation}
\label{eq:trigger_eff}
    f_{\mathrm{eff}} (x) = \frac{N(\left\{E_i|T_A(E_i) \wedge T_B(E_i), x)\right\}}{N(\left\{ E_i | T_A(E_i) = \mathrm{true} \right\} , x)}
\end{equation}

To determine the point, at which the trigger efficiency is larger than 99\%, the
turn-on distribution is fitted using a sigmoid function, that describes the
turn-on behaviour of the trigger paths.

\begin{equation}
\label{eq:trigger_eff_fit}
    f_{\mathrm{fit}} (x) = \frac{1}{2} \left( 1 + \erf \left(\frac{x-\mu}{\sqrt{2}\sigma}\right)\right)
\end{equation}

The thresholds which were finally used in the data analysis deviate slightly
from the ones shown in Figure~\ref{fig:trigger_eff} as each trigger threshold
was measured separately in each \ystar and \yboost bin respectively and the most
conservative, thus the highest threshold, is finally chosen. They are given in
Table~\ref{tab:triggers}

It has to be mentioned, that there are also HLT triggers available which
specifically trigger on the average transverse momentum of the dijet system.
These were implemented for dijet calibration purposes. Naturally, they appear to
be the better choice also for this analysis. The \ptavg trigger paths were studied,
but the performance is only comparable and partially even slightly worse due to
larger prescales applied to these trigger paths which result in larger
statistical uncertainties compared to the single jet trigger paths.

\subsection{Primary Vertex Selection}

The cuts on the primary vertices further reject beam background and off-centre bunch
crossings. Each event has to contain at least one primary vertex (PV) which is well
reconstructed within a distance of $|z_\mathrm{PV}| < \SI{24}{\centi \meter}$ to
the nominal interaction point of the detector. Futhermore, the radial distance
$\rho_\mathrm{PV}$ needs to be smaller than $\SI{2}{\centi\meter}$. To
ensure a high quality of the vertex reconstruction in the fit, the number of
degrees of freedom in the vertex fit, $n_{\mathrm{dof,PV}}$ needs to be at least
four. Consequently, at least four tracks need to be present in order to
perform a valid vertex fit.

\subsection{Missing transverse energy cut}

If all particles could be identified and perfectly measured, the transverse
momentum of all particles would sum up to zero. The imbalance in the transverse
momentum of all visible particles, which can be measured in the detector is
called the missing transverse momentum (MET). Neutrinos, for example, leave the
detector undetected and do contribute to the MET. MET is an important ingredient
in many measurements involving W bosons, top quarks or searches for physics
beyond the standard model which involve undetectable particles. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/measurement/metoversumet.pdf}
    \caption[Missing transverse energy distribution]{Missing transverse energy
    fraction of the total transverse energy per event in data and simulated
    events. To remove background and noise, events with a fraction exceeding a
    threshold are rejected. }
    \label{fig:mc:met_fraction}
\end{figure}

However, a large fraction of MET in an event is not always caused by physics
processes. Very often, the reason can be found in detector noise, cosmic rays or
beam-halo particles. Therefore, a sequence of algorithms developed by the MET
working group at CMS~\cite{jetmet:metfilters}, is employed which identifies and
rejects these events.  Moreover, a cut is applied which removes events in which
the missing transverse energy fraction \met constitutes a large fraction of the
total transverse energy $\sum_i E_{\mathrm{T},i}$, see
Fig.~\ref{fig:mc:met_fraction},

\begin{equation}
    \frac{\met}{\sum_i E_{\mathrm{T},i}} < 0.3.
\end{equation}

\subsection{Jet Identification}

The jet identification criteria (jet ID) rejects noise and noise-enhanced jets
while all real jets are kept. The jet ID is not applied event-wise, but each jet
is accepted or removed from the list of valid jets. The algorithm works on
reconstructed jets using information of the clustered particle candidates.
Following the official recommendations of the \textsc{JetMET}
group~\cite{jetmet:jetid}, the loose jet ID is used. All jets passing the jet ID
are then further processed in the analysis chain. 

The properties of the reconstructed jets and their respective cuts are listed in
Table~\ref{tab:jetid}. The cut on the fraction of neutral hadrons and photons
removes HCAL noise and ECAL noise, respectively. Muons, that are falsely
identified and clustered as jets are removed by the muon fraction criterion.
Based on information of the tracker, additional selection cuts are enforced in
the region $|\eta| < 2.4$. Fake jets clustered from electrons are removed by the
charged electromagnetic fraction cut. Furthermore, the fraction of charged hadrons in the
jet must be larger than zero. This cut is important since the CHS algorithm
removes charged particles from pileup vertices. Consequently, jets without any
charged hadrons are very likely to be pileup jets. The
Figs.~\ref{fig:jet_constituents_fractions} and~\ref{fig:jet_constituents_counts}
show the distributions of the jet constituents observed in data and simulated
events.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_chargedHadronFraction.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_electronFraction.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_muonFraction.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_neutralEMFraction.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_neutralHadronFraction.pdf}
    \caption{Control distribution for the jet constituent fractions. .}
    \label{fig:jet_constituents_fractions}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_nConstituents.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet_constituent_nCharged.pdf}
    \caption{Control distribution for the jet constituent fractions. .}
    \label{fig:jet_constituents_counts}
\end{figure}


While studying the loose and tight jet criteria, it was found that the tight jet
ID removes a non-negligible fraction of jets in the forward region of the
phase space considered in this analysis. Therefore, the loose jet ID is favored
and applied in this thesis as it is also the official recommendation of the
\textsc{JetMET} group.



\begin{table}[htbp]
    \centering
    \caption[Jet ID criteria]{The jet ID removes noise and fake jets based on
        the properties of the reconstructed jets and the clustered particle
        candidates. All selection cuts which are recommended by the
        \textsc{JetMET} group are applied~\cite{jetmet:jetid_twiki}. The loose jet ID is
        used in this analysis as the tight ID removes a non-negligible fraction
        of signal events, particularly in the forward region.}
    \label{tab:jetid}
    \begin{tabular}{llll}
    \toprule
                                 & \textbf{property}       & \textbf{loose ID} & \textbf{tight ID}\\\cmidrule(lr){2-4}
    \textbf{Whole $\eta$ region} &                         &                   & \\\cmidrule(lr){1-1}
                                 & neutral hadron fraction & $< 0.99$          & $< 0.90$\\
                                 & neutral EM fraction     & $< 0.99$          & $< 0.90$\\
                                 & number of constituents  & $> 1$             & $> 1$\\
                                 & muon fraction           & $< 0.8$           & $< 0.80$\\
    \textbf{only $|\eta| < 2.4$} &                         &                   & \\\cmidrule(lr){1-1}
                                 & charged hadron fraction & $> 0$             & $> 0$\\
                                 & charged multiplicity    & $> 0$             & $> 0$\\
                                 & charged EM fraction     & $< 0.99$          & $< 0.90$\\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Jet ID Efficiency}

The jet ID selection ensures a high purity of real jet events while keeping more
than 99\% of the real jets. The efficiency of the jet ID is studied using a
tag-and-probe method. 

Dijet events, in which the leading two jets are well balanced in $\phi$, are
selected using a $\Delta \phi > 2.7$ cut. While the tag jet is required to
fulfill the loose jet ID, the probe jet is checked if it required the ID as
well. The ratio of events where the probe
jet passes the criteria versus the number of events in which the probe doesn't
pass the critera is used to calculate the efficiency. Fig.~\ref{fig:jetid_eff}
shows the efficiency as a function of \ptavg of the dijet system for all bins of
the measurement. It is in all cases larger than 99\%.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jetideff_yb2ys0.pdf}
    \caption[Efficiency of the jet ID]{The jet ID efficiency is studied using a
    tag-and-probe approach on clean dijet event topologies. The efficiency is
    shown as a function of \ptavg for all bins. It is in all cases larger than 99\%.}
    \label{fig:jetid_eff}
\end{figure}

\subsection{Jet Correction and Selection}

The measurement is based on jets clustered from Particle Flow candidates using
the anti-\kt algorithm with a jet size parameter of $R=0.7$. Like the jet ID
selection, the following phase space cuts remove jets due to their transverse
momentum and rapidity. Consequently, all jet energy corrections recommended by
CMS are applied prior to this selection in order to have the correct energy
scale of the jets. The factorized correction approach employed by CMS is discussed in
Sec.~\ref{sec:jec} and comprise different correction levels for jets in data\footnote{The
JEC version applied on data is internally referred to as \texttt{Winter14\_V8}}
and for jets in simulated events\footnote{The latest JEC for run-independent
Monte Carlo Samples are called \texttt{START53\_V27}}.


The accessible phase space in theoretical calculations and the measurement is
synchronized by selecting jets only from a restricted part of the complete phase
space in which the detector acceptance is high and the applicability of NLO
pQCD calculations is guaranteed.

\begin{align*}
    \ptjet &> \SI{50}{\GeV}\\
    |y_\mathrm{jet}| &\leq 3.0\\
\end{align*}

Events, in which the leading or second jet is failing the jet selection, the
whole event is discarded to only keep event in which the two jets actually stem
from the hard scattering are selected. Furthermore, an additional cut on the
average transverse momentum of the dijets is applied. This cut is neccessary, as
the first jet trigger becomes efficient at this point.

\begin{align*}
    \ptavg &> \SI{133}{\GeV}
\end{align*}

Often, it is recommended to have asymmetric cuts on the transverse momentum of
the two leading jets to avoid an infrared sensitive region. This is not an issue
here because of the much higher cut on the average transverse momentum. Another
advantage of the high cut on \ptavg is the avoidance of a turn-on region which
would lead to complications in the applied unfolding procedure.

\subsection{Jet Angular Correction}

The jet energy corrections relate the measured jet energy to the true jet energy
on particle-level, but they do not include any correction for reconstruction
deficiencies of the jet pseudo-rapidity. However, especially in the transition
regions of the detector, \ie when the tracker coverage ends, a systematic
mismodelling is observed, see Fig.~\ref{fig:jet_eta_corr}. Jets are
reconstructed with systematically larger absolute pseudo-rapidity, \ie shifted towards
the forward region. While the absolute shift is rather small, it is a relevant
systematic effect when the rapidity separation of two jets is a observable.
However, two effects limit the impact of the mismodelling. At first, it is only
pronounced for jets with lower transverse momentum as can be seen in
Fig~\ref{fig:jet_eta_corr_vs_pt} and secondly, the large bin size in rapidity
reduces the effects. 

Nonetheless, the systematic error is accounted for by applying a correction on
each jet based on the average difference between the pseudo-rapidity of
particle-level jets and reconstructed jets as a function of reconstructed jet
\pt and pseudo-rapidity. The Figs.~\ref{fig:jet_eta_corr} and
~\ref{fig:jet_eta_corr_vs_pt} also show the dstribution after applying the
correction, in which the majority of the $\eta$ dependent effects is removed.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/genvsreco_eta.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/genvsreco_eta_corr.pdf}
    \caption[Differences of pseudo-rapidity of reconstructed jets to
        generator-level jets]{The difference between the pseudo-rapdity of the
            reconstructed jets and the generated jets is shown over the
            pseuudo-rapidity of the generated jet. The distribution is shown
            before (left) and after (right) applying the pseudo-rapidity
            correction. The blue points indicate the mean of the distribution in
            each $\eta_{\mathrm{gen}}$ bin.}
    \label{fig:jet_eta_corr}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/genvsreco_eta_vs_genpt.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/genvsreco_eta_vs_genpt_corr.pdf}
    \caption[Differences of pseudo-rapidity of reconstructed jets and particle-level jets as a function of the reconstructed jet \pt]
            {The pseudo-rapidity of the
            generated jets is shown as a function of the transverse momentum of
            the jets. The color indicates the differences of the pseudo-rapidity
            of the generated and reconstructed jets. The distribution is shown
            before (left) and after (right) applying the pseudo-rapidity
        correction.}
    \label{fig:jet_eta_corr_vs_pt}
\end{figure}

To estimate the effect on the resulting cross section, a comparison between the
yielded cross section with and without applying the pseudo-rapidity correction
is given in Fig.~\ref{fig:rap_corr_data}. As can be seen immediately, the effect is
rather small. In the measurement bins containing forward jets, the correction
causes about 2\% changes of the cross section. Since these events move to bins
containing more central jets and therefore have a much higher cross section, the
cross section increase is not noticeably.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/rap_corr_data_yb2ys0.pdf}
    \caption[Effect of pseudo-rapidity correction]{The effect of the
        pseudo-rapidity correction is demonstrated by showing the ratio of the
        cross section after applying the correction to the result without
        correction. The cross section decreases in bins involving forward jets
        and is more pronounced for higher values of \ystar.}
    \label{fig:rap_corr_data}
\end{figure}

\subsection{Stability versus Run Periods}

The experimental conditions for data-taking change slightly over the various run
periods, due to changes of the detector calibration or different trigger
prescales. Nonetheless the finally measured cross section must not depend on
these effects. This is checked by calculating the result separately for each run
period, see Fig.~\ref{fig:run_comparison}, which shows the ratio of the cross
section in each run period to the cross section obtained in run D. 

There are changes visible, most notably due to statistical fluctuations in the
high-\pt region. Futhermore, a slight slope of the cross section obtained in run
B is observed. However, in general all results are in good agreement and the
differences are negligible.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/run_comparison_yb2ys0.pdf}
    \caption[Stability of result over all run periods]{Ratio of the measured
    cross section in each run period to the cross section obtained with data
    from Run D. There are small differences, mostly due to statistical fluctuations.
    A slight slope is observed for the result of run B, but overall the differences
    are negligible.}
    \label{fig:run_comparison}
\end{figure}

\section{Comparison with Simulated Events}
\label{sec:simulated_events}

\subsection{Pileup Reweighting}

The official Monte-Carlo samples are enriched with an admixture of pileup
collisions to mimic the pileup distribution expected in data. Ideally, the
estimated pileup distribution in data $N_\mathrm{data} (N_\mathrm{PU, est.})$
would match with the simulated distribution $N_\mathrm{MC} (N_\mathrm{PU,
truth})$. Since the admixture is only a rough estimate of the pileup
distribution expected in the forthcoming data taking, a perfect matching cannot
be achieved. To still get comparable pileup distributions in data and simulated
events, the simulated events are reweighted with a weight $w_\mathrm{PU}$ to
match the distribution in data: 

\begin{equation*}
    w_{\mathrm{PU}} (N_{\mathrm{PU, truth}}) = \frac{N_\mathrm{data}
    (N_\mathrm{PU, est.}) / \sum N_\mathrm{data}}{N_\mathrm{MC}
    (N_\mathrm{PU, truth}) / \sum N_\mathrm{MC}}
\end{equation*}

Fig.~\ref{fig:mc:npv_reweighting} shows the number of reconstructed vertices
before and after reweighting. The significant mismatch of the
pileup distributions in data and simulated events, which can be observed before
reweighting the simulated events, vanishes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/npv_beforereweighting.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/npv_afterreweighting.pdf}
    \caption[Number of reconstructed vertices]{Number of reconstruced vertices in data and simulated events before
    (left) and after (right) the pileup reweighting.}
    \label{fig:mc:npv_reweighting}
\end{figure}

\subsection{Control distributions}

Figure~\ref{fig:controlplots:properties} show the numerous energy fractions of the jet constituents. The top
row of this figure show the charged and neutral fraction of hadrons in the jet. The middle row shows
the fraction of muons and photons within the jet while the bottom row show the electron fraction and
the HF hadron fraction.



\subsection{Comparison of Kinematic Quantities}

The measured data distributions on reconstructed level are compared with simulated events
processed through the whole event generation and detector simulation. In this section
the basic kinematic quantities of the leading two jets are compared as well as the properties
of each of the jets mainly describing the constituent fractions of each jet. Figure~\ref{fig:controlplots:kinematic}
shows the transverse momentum $\pt$, the rapidity $y$ and the azimuthal angle $\phi$ for the leading
two jets. The kinematic of the two dijet system is well described by the MC simulation apart from
a few regions with larger discrepancies. The \pt-distribution is not that well described especially in
the lower \pt region and the number of events with forward jets is overestimated in the MC simulation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet1pt_default.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet2pt_default.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet1rap_default.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet2rap_default.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet1phi_default.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet2phi_default.pdf}
    \caption[Control plots of kinematic quantities of the dijet system]{The plots show the basic kinematic quantities of the two leading jets in a selected event.
             The transvers momentum of the leading (left plot) and the second (right plots) jet is shown
         in the top row. The rapidity of the jets is shown in the middle row. The azimuthal angle of the
     leading jet and the second jet is shown on the bottom row.}
    \label{fig:controlplots:kinematic}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/jet12_rapidity_yb2ys0.pdf}
    \caption[Rapidities of the two leading jets in the various \ystar and \yboost bins]{
             The distribution of the dijet events in the various \ystar and
             \yboost bins is shown as a function of the rapdities of the two jets.
             The color indicates the frequency distribution. The figures demonstrate the
             division of SS and OS dijet events, see \eg mid left and bottom right plot.}
    \label{fig:controlplots:rapidity}
\end{figure}

\subsection{Cross section Comparison}

The Monte-Carlo simulation is used to compare the prediction including the detector simulation
to the measured data distribution which is smeared due to the finite detector resolution. Figure~\ref{fig:ratio_recotodata}
show the prediction of the MG+P6 and the P8 simulations as a ratio to the measured data spectrum.
While the agreement in the inner detector region is fine, the shape and especially the normalization
of the MC prediction cannot describe the data. Additionally the fixed order prediction of NLOJET++
is shown. This prediction is on parton level and is not corrected for non-perturbative effects. Still
it describes best the data apart from the known effects due to NP and detector resolution effects.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_reco_to_data_yb2ys0.pdf}
    \caption[Comparison of data with simulated events]{Comparison of the measurement and the predictions using P8 and MG+P6. The data distributions are normalized
    to the integrated luminosity, the simulated events to the number of generated events and the cross section. The ratio of the simulated
    events to the data points is shown.}
    \label{fig:ratio_recotodata}
\end{figure}


\section{Dijet Transverse Momentum  Resolution}
\label{sec:resolution}

The true transverse momentum of the jets measured in the CMS detector is smeared through the finite
resolution. In order to correct the finally measured cross section for these
detector effects, the energy resolution of the obserable has to be determined.
As the simulated events are propagated through the detector simulated, the
information both at particle-level and at reconstructed level is available. To
compare particle-level jets and reconstructed jets, they have to be matched to
each other. The distance $\Delta R$ in the $\eta$-$\phi$ plane is calculated as

\begin{equation}
\Delta R = \sqrt{\Delta \eta^2 + \Delta \phi^2}
\end{equation}
%
and needs to satisfy $\Delta R < 0.3$, roughly half the jet size parameter
$R=0.7$. The jets closest in the $\eta$-$\phi$ space are then matched. However,
studies of the \textsc{JERC} working group~\cite{jetmet:resolution} turned out that the resolution
determined in simulated events is better than the resolution observed in data
using data-driven methods. Therefore, the jet transverse momentum in simulated
events is additionally smeared in order to match the resolution in data.
Table~\ref{tab:res_smearing} gives these smearing factors together with the
assigned uncertainty, indicated by the up- and downwards variations. To consider
the detector geometry, the smearing factors are derived separately for multiple
$|\eta|$-regions.

\begin{table}[htbp]
\setlength\tabcolsep{4.5pt} 
    \centering
    \caption[Jet energy resolution scale factors]{
             The jet energy resolution in data is significantly worse than in
             simulated events. Therefore, the reconstructed jet tansverse
             momentum in simulated events is smeared using a factor $c$ to
             effectively match the resolution in data, following the
             recommendations of \JetMET group of CMS~\cite{jetmet:resolution}.
             The uncertainty on the resolution is given by an upwards and
             downwards variation $c_\mathrm{up}$ and
             $c_\mathrm{down}$ of the smearing factor.}
    \label{tab:res_smearing}

    \begin{tabular}{lccccccr}
    \toprule
    & & \multicolumn{6}{c}{$\bm{|\eta|}$}\\\cmidrule{2-8}
               & $0.0$ -- $0.5$ & $0.5$ -- $1.1$ & $1.1$ -- $1.7$ & $1.7$ --
               $2.3$ & $2.3$ -- $2.8$ & $2.8$ -- $3.2$ & $3.2$ -- $5.0$\\
               $\bm{c}$ &  &  &  & & & & \\\cmidrule{1-1}
    nominal    & 1.079                   & 1.099                   & 1.121    & 1.208                   & 1.254                   & 1.395    & 1.056\\
    downward       & 1.053                   & 1.071                   & 1.092    & 1.162                   & 1.192                   & 1.332 & 0.865\\
    upward         & 1.105                   & 1.127                   & 1.150 & 1.254                   & 1.316                   & 1.458 & 1.247\\
    \bottomrule
    \end{tabular}
\end{table}

The smearing of the reconstructed jet \pt is performed as a multiplicative scale
factor based on the difference of \ptreco and \ptgen.

\begin{equation}
\ptreco = \max \left( 0, \ptgen + c(\eta) \cdot (\ptreco - \ptgen) \right)
\end{equation}
%
After smearing, the response defined as

\begin{equation}
    R = \frac{\ptavg^{\text{reco}}}{\ptavg^{\text{gen}}},
\end{equation}

is calculated. Figure~\ref{fig:gen_vs_reco_over_gen} shows the two-dimensional
distributions for each bin. As expected, the resolution of low-\pt is
significantly worse than the one of high-\pt jets. As the response is dependent
on both detector region and the transverse momentum of the jets, the resolution
is calculated as a function of \ptavggen separately in each \ystar and \yboost
bin. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/gen_vs_reco_vs_gen_ptavg_yb2ys0.pdf}
    \caption[Comparison generated vs. reconstructed transverse energy]
            {The response of reconstructed and particle-level jet transverse
                momentum as a function of the particle-jet transverse momentum.
                The width of the distribution in each \ptavg bin indicates the
             jet energy resolution, which improves from lower to higher higher values of \ptavg. The
             resolution is extracted separately for each bin and finally
         paramterized using the NSC-formula.}
    \label{fig:gen_vs_reco_over_gen}
\end{figure}

The resolution is now determined as the width of the distribution in each \ptavg
bin, as can be seen in Fig.~\ref{fig:resolution_bin}. Both a gaussian function
as well as a double-sided crystal ball function have been studied in order to
describe the observed behaviour. Despite the large differences in the
description of the non-gaussian tails, enhanced by the logarithmic
representation, the determined width of the distribution is very similar.
Nonetheless, the crystal ball function is used to determine the resolution, as
it better describes the measured distributions, especially in the low-\pt
region.

\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/measurement/resolution_yb0ys0_bin10.pdf}\hfill
    \includegraphics[width=0.47\textwidth]{figures/measurement/resolution_yb0ys0_bin10_cb.pdf}
    \caption[Gaussian and crystal ball fit of resolution.]{The resolution in
        each \ptavg bin is fitted using a gaussian (left) and a double-sided crystal ball
        (right) function. The fit using the crystal ball function better describes the
        non-gaussian tails of the distribution. Therefore, it is favored in the
    determination of the jet energy resolution determination.}
    \label{fig:resolution_bin}
\end{figure}

Fig.~\ref{fig:resolution_ptavg} shows the resolution in each \ystar and
\yboost bin as a function of \ptavg. The relative resolution is fitted using a
modified version of the NSC formula.


\begin{equation}
    \frac{\sigma_{\text{ptavg}}}{\ptavg} (\ptavg) = \sqrt{\sgn{N} \left(\frac{N}{\ptavg}\right)^2 + \left(\frac{\ptavg}{\text{GeV}}\right)^s \frac{S^2}{\ptavg} + C^2 }
    \label{eq:resolution}
\end{equation}
%
The formula is based on the common NSC formula which describes the resolution in
terms of noise $N$, a stochastic component $S$ and a constant term $C$.
Especially in the low-\pt region in which the tracking has a non-negligible
influence on the resolution due to the particle flow algorithm, a slightly
better fit is obtained by using the modified resolution
formula~\ref{eq:resolution}. The influence of the different resolution formula
on the unfolded cross section however is negligible, as the spectrum starts at
much higher \pt. Table~\ref{tab:resolution_parameters} gives the parameters of
the fit in each \ystar and \yboost bin of the measurement.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/measurement/resolution_ptavg_crystalball.pdf}
    \caption[Relative jet resolution vs \ptavg]{The jet energy resolution as a
        function of \ptavg is shown for all measurement bins. The data points
        indicate the separate determinations while the solid lines give the
    result of the fit using the NSC-formula.}
    \label{fig:resolution_ptavg}
\end{figure}


\begin{table}[htbp]
    \centering
    \caption[Relative dijet transverse momentum resolution parameters]
    {Fitted paramameters of the modified NSC formula describing the transverse
    momentum resolution in the individual \ystar and \yboost bins.}
    \label{tab:resolution_parameters}
    \begin{tabular}{cccccc}
        \toprule
         \yboost & \ystar & N      & S     & C      & s\\\midrule
         0 -- 1  & 0 -- 1 & -2.68  & 1.43  & 0.03   & -0.26\\
         0 -- 1  & 1 -- 2 & -8.00  & 5.81  & 0.04   & -0.73\\
         0 -- 1  & 2 -- 3 & -0.02  & 1.98  & -0.018 & -0.36\\
         1 -- 2  & 0 -- 1 & -8.13  & 5.96  & 0.04   & -0.73\\
         1 -- 2  & 1 -- 2 &  2.85  & 1.16  & 0.02   & -0.17\\
         2 -- 3  & 0 -- 1 &  3.96  & 1.23  & 0.00   & -0.18\\\hline
            \bottomrule
    \end{tabular}
\end{table}

\section{Unfolding of the Measurement}
\label{sec:unfolding}

The final goal of this analysis is the comparison of the triple-differential
dijet cross section measurement with higher-order QCD calculations on
particle-level. Due to finite detector acceptance and resolution, the jet
transverse momentum is smeared causing differences between the particle-level
\ptavg spectrum and the measured spectrum. To allow particle-level comparisons,
the measurement has to be unfolded. Thus, calculations from future Monte-Carlo
event generators can be easily compared with this measurement without the need to
apply the detector simulation.

In this analysis, the iterative d'Agostini algorithm~\cite{DAgostini:1994zf}
which is implemented in the RooUnfold~\cite{Adye:2011gm} package is used. The
unfolding process is regularized by the number of interation steps in this
algorithm. A higher number of iterations yields a better reduced \chisq but also
increases the uncertainty and introduces larger bin-by-bin fluctuations and
correlations.  The regularization is optimized using a Monte-Carlo data sample
and best results with low bin-by-bin correlations and a low \chisq were achieved
using four iterations in the unfolding algorithm.

In principal, the response matrix for the unfolding algorithm can be populated
directly using simulated events as they contain both the particle-level and
reconstructed jets. However, this method has several drawbacks. The LO
prediction does not describes the shape of the distribution perfectly in all
bins. Moreover, the limited number of events in the Monte-Carlo sample, especially at high
rapidities and high transverse momenta, introduces non-negligible statistical
fluctuations in the response matrix. Because of these undesired effects an
alternative stategy is employed to populate the response matrix.

Relying on the good description of the data spectrum by NLO predictions, a forward
smearing technique is applied. The NLO prediction, obtained with the CT14-NLO PDF
set and corrected for non-perturbative effects, is fitted using the function

\begin{equation}
    f(\ptavg) = A_0 \left(
    \frac{\ptavg}{A_3}\right)^{-A_1}\left(1-\frac{\ptavg}{A_3}\right)^{A_2},
\end{equation}

which describes both the normalization and shape of the distribution. Using toy
Monte Carlo events, a flat \ptavg spectrum is generated, which is weighted
according to the fitted NLO distribution. The distribution of this generated
sample resembles the fastNLO cross section prediction and is used as the
particle-level \ptavg spectrum. All generated events are also smeared using the
resolution function obtained in Section~\ref{sec:resolution}. The generation of
these Monte-Carlo toy events is very fast, and the response matrices can be
filled with around 100 million events each, resulting in negligible statistical
fluctuations. Figure~\ref{fig:res_matrix} shows the response matrices used in
the unfolding process. The matrices in the figure are normalized to the number
of particle-level entries in each row to improve the readibility. The response
matrices are diagonal with small off-diagonal migrations between neighboring
\ptavg bins.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/res_matrix_ptavg_normalized_yb2ys0.pdf}
    \caption[Response matrix used for the unfolding procedure]{Response matrix
        used for the unfolding procedure. The response matrices are obtained
        using the smearing method and are normalized to the number of gen events
        in each row to improve the readibility.}
    \label{fig:res_matrix}
\end{figure}

A closure test was performed by performing the unfolding using response matrices
filled directly from the simulated events of the Madgraph data sample and
filled using the forward smearing technique. It was found that the results agree
in both cases within statistical uncertainties which are dominated by the
uncertainties of the Madgraph sample. It has to be noted, that these
techniques do not consider fluctuations between \ystar and \yboost bins.
However, migrations between neighbouring \ystar and \yboost bins were studied
and and found to be negligible. Therefore, it is justified to perform the
unfolding in each \ystar and \yboost bin separately and to not perform a
three-dimensional unfolding which is much more cumbersome error-prone.

Statistical uncertainties of the data distributions are propagated through
the unfolding procedure using toy experiments. The procedure as well as its
results are discussed in detail in Sec.~\ref{sec:stat_unf_uncert} when the
experimental uncertainties are discussed. The uncolded cross sections are shown
in Sec.~\ref{sec:nlo_comparisons} in which the comparison to NLO calculations is presented.

\section{Experimental Uncertainties}
\label{sec:experimental_uncertainties}

In this section, all experimental sources of uncertainty which affect the cross
section measurement are discussed. The comprehensive studies involve five
sources of uncertainty of wich the jet energy correction uncertainties were
found to be the dominant. Statistical uncertainties were propagated through the
unfolding procedure while taking into account correlations due to bin
migrations. The imperfection of the CMS luminosity measurement causes a
normalization uncertainty. The uncertainty of the jet energy resolution
determination is also propagated to the unfolded cross section. Residual
systematic errors due to jet ID and trigger efficiencies are very small and
covered by 1\% uncorrelated uncertainty.

Fig.~\ref{fig:exp_unc_overview} depicts all sources of experimental uncertainty
in combination with the total experimental uncertainty, obtained by adding in
quadrature all individual sources. The total uncertainty is 8\% in the
measurement bins involving jets at central rapidity and increases up to 20\% in
bins involving jets in worse understood phase space regions.

\todo{table exp uncertainties}

\subsection {Uncertainty on Luminosity Measurement}
\label{sec:luminosity_uncertainty}

As discussed in Sec.~\ref{sec:lumi_measurement}, the luminosity is measured from
the number of clusters in the silicon pixel detector. The integrated luminosity
for certified data of the 2012 LHC run is measured to be \SI{19.71}{\per \femto
\barn}. The uncertainty on the luminosity measurement for the 2012 LHC run is
estimated to be 2.5\% (syst.) and 0.5\% (stat.)~\cite{CMS-PAS-LUM-13-001}. As
the luminosity uncertainty afffects the normalization of any absolute cross
section measurement, a combined systematic uncertainty of 2.6\% is assigned.

\subsection{Unfolding and Statistical Uncertainty}
\label{sec:stat_unf_uncert}

Statistical uncertainties of data points are propagated through the unfolding
using a toy MC technique in which the measured data points are smeared within
their statistical uncertainties and the unfolding procedure is performed
multiple times for each smeared spectra. A million such toy spectra are used to
propagate the statistical uncertainty.  Figure~\ref{fig:statunc_relative} shows
the relative statistical uncertainty before and after the unfolding procedure.
The uncertainty slightly increases during the unfolding process which is
expected.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/statunc_fractional_yb2ys0.pdf}
    \caption[Statistical uncertainty of measured and unfolded sprectrum]{The
    statistical uncertainties of the measured and the unfolded spectrum. Due to
    the unfolding procedure, the uncertainties slightly increase compared to the
    measured spectrum.}
    \label{fig:statunc_relative}
\end{figure}

Futhermore, the unfolding introduces a correlation between bins due to event
migrations. These correlations are significant for neighbouring bins in \pt and
negligible between bins far off in the phase space.
Figure~\ref{fig:corr_unfolding_nlo} shows the correlations of the statistical
uncertainty after the unfolding. Of course, these correlations have to be taken
into account in a statistical analysis of the data, such as in a PDF fit.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/unf_nlo_corr_yb2ys0.pdf}
    \caption[Correlations of statistical uncertainty]{Correlation of the
        statistical uncertainty introduced by the unfolding procedure.
        Neighbouring bins have a significant correlation or anti-correlation
        through bin-by-bin migrations.}
    \label{fig:corr_unfolding_nlo}
\end{figure}

\subsection{Jet Energy Correction Uncertainty}

The dominant part of the experimental uncertainties comes from jet energy
calibration, which correct the measured jet energy for a variety of detector
effects, see Sec.~\ref{sec:jec}. As the corrections are afflicted with multiple
sources of systematic uncertainty, these are propagated individually to the cross section
to preserve all correlations.

The JEC uncertainties are split into 25 mutually independent sources of
uncertainty (JES), in which each source is fully correlated in \pt and $\eta$
but uncorrelated to all other sources. Each variation presents a $1\sigma$
shift. As these uncertainties can be asymmetric, the upwards and downwards
variation of each source are treated seperately. The sum in quadrature of all uncertainty
sources equals the total uncertainty. Therefore they can be treated exactly like
the PDF eigenvector sets, which were discussed in
Sec.~\ref{sec:pdf_uncertainties}. The sources of uncertainties are grouped in
four categories which relate to the underlying correction. In the following
list, a short summary of the sources of uncertainty is given. The calibration
procedure and the uncertainties are described in great detail
in~\cite{jec_paper}. The Figs.

\begin{description}
    \item[Pileup JES] Differences in the transverse momentum between the true
        offset and the random cone offset are observed in simulated events. This
        difference is propagated using Z/$\gamma$+jet and dijet balancing
        methods to estimate the residual pileup uncertainty after the
        calibration.
    \item[Relative JES] The relative $\eta$-dependent corrections calibrate
        forward jets using balanced dijet events. The largest contribution to
        the uncertainty arises from jet energy resolution and soft-radiation
        bias corrections. 
    \item[Absolute JES]  The absolute calibration of the jet energy scale relies
        on Z/$\gamma$+jet and multi-jet events. The uncertainties are related to
        the lepton momentum scale for muon and the single-pion response in the
        HCAL. Observed differences in applied methods can be traced back to neutrinos
        and ISR and are accounted for in these sources of uncertainty.
    \item[Flavor JES] Differences in the flavor response are studied using
        simulation by cross-checking the results with quark- and gluon-tagged
        $\gamma$+jet and Z+jet events. The uncertainty is derived based on
        differences observed in the Pythia 6 and Herwig++ simulation.
\end{description}

\subsection{Jet Energy Resolution Uncertainty}

The jet energy resolution, as derived in Sec.~\ref{sec:resolution}, influences
the unfolding and consequently introduces a further source of uncertainty on the
unfolded cross section. 

Table~\ref{tab:res_smearing} shows the smearing factors, which were applied on
MC to obtain the actual resolution in data. The official recommendations involve
an uncertainty on this smearing factor to estimate an uncertainty on the
determined resolution, see the varied smearing factors in
Table~\ref{tab:res_smearing}. The determination of the resolution is repeated
with the up- and down-wards variation of the resolution smearing factor $c$
applied. The unfolding procedure is also reiterated using the variations of the
resolution and the differences of the obtained cross section to the nominal
cross section are accounted for as a systematic uncertainty. The uncertainty on
the cross section is comparably small, about 1\% at low rapidity and increasing
up to 3\% at larger rapidities, see Fig.~\ref{fig:exp_unc_overview}.

\todo[inline]{uncertainty overview small paragraph}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/exp_unc_overview_yb2ys0.pdf}
    \caption[Overview of experimental uncertainties]{Overview of all
    experimental uncertainties affecting the cross section measurement. The
    errorbars indicate the statistical uncertainty after unfolding. The colored
    lines give the uncertainties resulting of jet energy corrections, jet energy
    resolution, luminosity and residual effects. The total uncertainty is yielded by
    adding in quadrature the individual sources of uncertainty.}
    \label{fig:exp_unc_overview}
\end{figure}

\section{Comparison with NLO Predictions}
\label{sec:nlo_comparisons}

After unfolding the measurement to particl-level, it is finally possible to
compare the cross sections with the NLO calculations which were obtained in
Sec.~\ref{sec:theory_predictions}. A general comparison is given in
Fig.~\ref{fig:measurement_result} which shows the data overlayed with the NLO
theory prediction obtained with the CT14-NLO PDF set. The measurement and the
prediction agree over eight orders of magnitude in cross section and up to
highest transverse momenta.

\begin{figure}[h!tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/measurement/ptavg_spectrum.pdf}\hfill
    \caption[Spectrum of the Triple-differential Dijet Cross Section]{The
    triple-differential dijet cross section in six bins of \ystar and \yboost. The
    data is indicated by different markers for each bin and the theory obtained
    with CT14-NLO is depicted by colored lines. Good agreement between data and
    NLO calculation is observed over many order of magnitude.}
    \label{fig:measurement_result}
\end{figure}

A more detailed comparison is achievable when the ratio of the data to theory is
calculation. Figures~\ref{fig:ratio_ct14_nlo}---\ref{fig:ratio_nnpdf30_nlo}
present such a ratio including all statistical and systematic uncertainties. To
emphasize the sensitivity of PDFs to this observable, calculations using the
newest CT14, NNPDF 3.0, MMHT 2014 and ABM 11 PDF sets, all derived at NLO, are
shown. Apart from the bins with high values of \yboost, the predictions of most
PDF sets coincide within uncertainties. However, the ABM 11 predictions
systematically underestimate the data. This behaviour of ABM 11 PDF set is well
known and seen in a multidue of other jet measurements. It is due to the
combination of a soft gluon PDF accompanied with a low value of \asmz in the PDF
set.

A further effect in which all theory predictions differ from the data, is
prominently visible in the central region at large transverse momenta. Here, the
theory predictions underestimate the measured data by up to 20\%. This effect
has been observed earlier and can be attributed to the neglection of
electroweak corrections which are positive and sizeable at low rapidity and high
transverse momenta.  Theory colleagues are currently working on providing the
electroweak corrections specifically for this measurement, but they were not
available in time\todo[inline]{check for ewk}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_CT14nlo+np_totcomp_yb2ys0.pdf}
    \caption[Ratio of the cross section to CT14 NLO]{
    Ratio of the triple-differential dijet cross sections to the theoretical
    prediction using the central value of the CT14 NLO PDF set for each bin in \ystar
    and \yboost respectively. The datapoints including statistical uncertainty are
    indicated by markers, the total experimental uncertainty is represented by the
    hatched red band. The solid blue band indicates the PDF uncertainty and the
    continous colored lines the predictions of the cross sections calculated with
    other PDF sets. }
    \label{fig:ratio_ct14_nlo}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb0ys0.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb0ys1.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb0ys2.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb1ys0.pdf}
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb1ys1.pdf}\hfill
    \includegraphics[width=0.49\textwidth]{figures/measurement/ratio_to_NNPDF30+np_totcomp_yb2ys0.pdf}
    \caption[Ratio of the cross section to NNPDF 3.0 NLO]{
    Ratio of the triple-differential dijet cross sections to the theoretical
    prediction using the central value of the NNPDF 3.0 NLO PDF set for each bin in \ystar
    and \yboost respectively. The datapoints including statistical uncertainty are
    indicated by markers, the total experimental uncertainty is represented by the
    hatched red band. The solid blue band indicates the PDF uncertainty and the
    continous colored lines the predictions of the cross sections calculated with
    other PDF sets. }

    \label{fig:ratio_nnpdf30_nlo}
\end{figure}

Especially phase space regions, in which the data discriminates between the
predictions of different PDF sets, are interesting input for PDF studies. As
discussed in Sec.~\ref{sec:crosssection_definition}, the bins involving boosted
same-side dijet events, in which the fractional proton momenta $x_1$ and $x_2$
are very different, are predestined for PDF studies. The predictions of the
different PDF sets, especially the NNPDF PDF set, yield different results and
are afflicted with a large PDF uncertainty. Moreover, none of the investigated
PDF sets yields a good description of the data in this phase space region, see
bottom right plot in Fig.~\ref{fig:ratio_ct14_nlo}.



